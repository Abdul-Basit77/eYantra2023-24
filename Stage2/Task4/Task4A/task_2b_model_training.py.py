# -*- coding: utf-8 -*-
"""3000_training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KWFxnYH397FusNnLHEdw6yFHFCzxcaTz
"""

import tensorflow as tf
import os
import numpy as np
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount("/content/drive")

Train_path="/content/drive/MyDrive/eyantra/task2b/try_eyantra/resized_training/"

os.listdir(Train_path)

Total_images=tf.keras.utils.image_dataset_from_directory(Train_path,image_size=(224,224))

Scaled_Total_images=Total_images.map(lambda x,y:(x/255,y))
Scaled_image_iterator=Scaled_Total_images.as_numpy_iterator()
train_size = int(len(Scaled_Total_images)*.8)
val_size = int(len(Scaled_Total_images)*.2)
train = Scaled_Total_images.take(train_size)
test = Scaled_Total_images.skip(train_size).take(val_size)

feature_extracter_model="https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4"

import tensorflow_hub as hub
pretrained_model_without_top_layer=hub.KerasLayer(feature_extracter_model,input_shape=(224,224,3),trainable=False)

no_of_classes=5
model=tf.keras.Sequential([
    pretrained_model_without_top_layer,
    tf.keras.layers.Dense(no_of_classes,activation="softmax")
])
model.summary()

model.compile(optimizer="adam",loss=tf.keras.losses.SparseCategoricalCrossentropy(),metrics=["accuracy"])

model.fit(train,epochs=5)

model.evaluate(test)

Eval_path="/content/drive/MyDrive/eyantra/task2b/try_eyantra/testing/"
Total_Test_images=tf.keras.utils.image_dataset_from_directory(Train_path,image_size=(224,224))
Scaled_Total_Test_images=Total_images.map(lambda x,y:(x/255,y))

model.evaluate(Scaled_Total_Test_images)

model.save("/content/drive/MyDrive/eyantra/task2b/try_eyantra/resizemodel3000.h5")

from google.colab.patches import cv2_imshow

import cv2
arena_path="/content/drive/MyDrive/eyantra/task2b/try_eyantra/sample.png"
frame = cv2.imread(arena_path)
arena = cv2.resize(frame, (700, 700))

event_list = []
def event_identification(arena):        # NOTE: You can tweak this function in case you need to give more inputs

    square_coordinates = [(152, 110), (139, 328), (458, 327), (453, 459), (149, 588)]

    # Loop through the coordinates and extract the 64x64 squares
    for x, y in square_coordinates[::-1]:

        square = arena[y:y+70, x:x+70]

        cv2_imshow(square)
        event_list.append(cv2.resize(square,(224,224)))
event_identification(arena)

combat = "combat"
rehab = "humanitarianaid"
military_vehicles = "militaryvehicles"
fire = "fire"
destroyed_building = "destroyedbuilding"
detected_list=[]
def event_name(event_key):
    #2fire,4militaryvehicles,3aid,0combat,1destroyed
    name_dict={0:"combat",1:"destroyedbuilding",2:"fire",3:"humanitarianaid",4:"militaryvehicles"}
    return name_dict[event_key]
def classify_event(image):
    # image=cv2.medianBlur(image,3,3)
    # image=cv2.GaussianBlur(image,(3,3),0)
    # image=cv2.addWeighted(image1,1.5,image,-0.5,0)
    event_key=np.argmax(model.predict(np.expand_dims(image/255, 0)))
    event = event_name(event_key)
    return event
def classification(event_list):
    for img_index in range(0,5):
        img = event_list[img_index]
        detected_event = classify_event(img)
        print((img_index + 1), detected_event)
        if detected_event == combat:
            detected_list.append("combat")
        if detected_event == rehab:
            detected_list.append("rehab")
        if detected_event == military_vehicles:
            detected_list.append("militaryvehicles")
        if detected_event == fire:
            detected_list.append("fire")
        if detected_event == destroyed_building:
            detected_list.append("destroyedbuilding")
    #os.remove('arena.png')
    return detected_list


detected_list=classification(event_list)
print(detected_list)

